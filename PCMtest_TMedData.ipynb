{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying PCM with a new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray\n",
    "import numpy as np\n",
    "import pyxpcm\n",
    "from pyxpcm.models import pcm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset: CMCC temperature data run (20200611)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ref-oc-intranet/cmcc_mfseas4/best_estimate/2020/CMCC_MFSEAS-4_20200611_TEMP.nc'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_path = \"/home/ref-oc-intranet/cmcc_mfseas4/best_estimate/2020/CMCC_MFSEAS-4_20200611_TEMP.nc\"\n",
    "ds_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xarray.open_dataset(ds_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PCM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dictionary to define the list of features and their vertical axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.01823664e+00, -3.16574740e+00, -5.46496344e+00, -7.92037725e+00,\n",
       "       -1.05366039e+01, -1.33183842e+01, -1.62705860e+01, -1.93982105e+01,\n",
       "       -2.27063923e+01, -2.62003994e+01, -2.98856430e+01, -3.37676735e+01,\n",
       "       -3.78521919e+01, -4.21450386e+01, -4.66522102e+01, -5.13798599e+01,\n",
       "       -5.63342857e+01, -6.15219574e+01, -6.69494934e+01, -7.26236877e+01,\n",
       "       -7.85514984e+01, -8.47400436e+01, -9.11966324e+01, -9.79287262e+01,\n",
       "       -1.04943977e+02, -1.12250206e+02, -1.19855431e+02, -1.27767838e+02,\n",
       "       -1.35995804e+02, -1.44547897e+02, -1.53432846e+02, -1.62659622e+02,\n",
       "       -1.72237350e+02, -1.82175354e+02, -1.92483139e+02, -2.03170441e+02,\n",
       "       -2.14247162e+02, -2.25723404e+02, -2.37609467e+02, -2.49915848e+02,\n",
       "       -2.62653229e+02, -2.75832520e+02, -2.89464783e+02, -3.03561310e+02,\n",
       "       -3.18133545e+02, -3.33193146e+02, -3.48751953e+02, -3.64821960e+02,\n",
       "       -3.81415436e+02, -3.98544708e+02, -4.16222321e+02, -4.34461060e+02,\n",
       "       -4.53273773e+02, -4.72673492e+02, -4.92673462e+02, -5.13286987e+02,\n",
       "       -5.34527588e+02, -5.56408875e+02, -5.78944580e+02, -6.02148621e+02,\n",
       "       -6.26034912e+02, -6.50617554e+02, -6.75910706e+02, -7.01928650e+02,\n",
       "       -7.28685608e+02, -7.56196045e+02, -7.84474304e+02, -8.13534851e+02,\n",
       "       -8.43392151e+02, -8.74060669e+02, -9.05554810e+02, -9.37889099e+02,\n",
       "       -9.71077881e+02, -1.00513550e+03, -1.04007629e+03, -1.07591431e+03,\n",
       "       -1.11266370e+03, -1.15033838e+03, -1.18895215e+03, -1.22851880e+03,\n",
       "       -1.26905176e+03, -1.31056421e+03, -1.35306934e+03, -1.39657996e+03,\n",
       "       -1.44110864e+03, -1.48666785e+03, -1.53326941e+03, -1.58092517e+03,\n",
       "       -1.62964661e+03, -1.67944482e+03, -1.73033032e+03, -1.78231360e+03,\n",
       "       -1.83540454e+03, -1.88961267e+03, -1.94494714e+03, -2.00141663e+03,\n",
       "       -2.05902905e+03, -2.11779248e+03, -2.17771411e+03, -2.23880029e+03,\n",
       "       -2.30105762e+03, -2.36449170e+03, -2.42910767e+03, -2.49491016e+03,\n",
       "       -2.56190308e+03, -2.63008984e+03, -2.69947363e+03, -2.77005664e+03,\n",
       "       -2.84184082e+03, -2.91482690e+03, -2.98901587e+03, -3.06440747e+03,\n",
       "       -3.14100146e+03, -3.21879614e+03, -3.29779028e+03, -3.37798145e+03,\n",
       "       -3.45936621e+03, -3.54194189e+03, -3.62570386e+03, -3.71064746e+03,\n",
       "       -3.79676807e+03, -3.88405957e+03, -3.97251611e+03, -4.06213037e+03,\n",
       "       -4.15289600e+03, -4.24480420e+03, -4.33784766e+03, -4.43201758e+03,\n",
       "       -4.52730420e+03, -4.62369873e+03, -4.72119141e+03, -4.81977100e+03,\n",
       "       -4.91942725e+03, -5.02014941e+03, -5.12192578e+03, -5.22474463e+03,\n",
       "       -5.32859375e+03, -5.43346094e+03, -5.53933350e+03, -5.64619922e+03,\n",
       "       -5.75404395e+03], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# z = np.arange(0.,-1000,-10.)\n",
    "# z = np.linspace(min(ds.depth.values),max(ds.depth.values), num = ds.depth.size)\n",
    "z = ds.depth.values\n",
    "z = -z\n",
    "pcm_features = {'temperature': z}\n",
    "z\n",
    "#print(z)\n",
    "#print(pcm_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<title>Show/Hide data repr</title>\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<title>Show/Hide attributes</title>\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt, dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2 {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><div class='xr-wrap'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-c16fb271-f6eb-4b26-9b97-0d9fdc2ed2d6' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-c16fb271-f6eb-4b26-9b97-0d9fdc2ed2d6' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>depth</span>: 141</li><li><span class='xr-has-index'>lat</span>: 380</li><li><span class='xr-has-index'>lon</span>: 1287</li><li><span class='xr-has-index'>time</span>: 1</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-35ca522c-78bc-4b57-bf58-f558d0dc3958' class='xr-section-summary-in' type='checkbox'  checked><label for='section-35ca522c-78bc-4b57-bf58-f558d0dc3958' class='xr-section-summary' >Coordinates: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>depth</span></div><div class='xr-var-dims'>(depth)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>-1.0182366 -3.1657474 ... -5754.044</div><input id='attrs-bad4f651-5875-4452-aefe-1cb093f88120' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-bad4f651-5875-4452-aefe-1cb093f88120' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-c1fdb2af-731e-46ca-9d84-0803f713eba9' class='xr-var-data-in' type='checkbox'><label for='data-c1fdb2af-731e-46ca-9d84-0803f713eba9' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><pre class='xr-var-data'>array([-1.018237e+00, -3.165747e+00, -5.464963e+00, -7.920377e+00,\n",
       "       -1.053660e+01, -1.331838e+01, -1.627059e+01, -1.939821e+01,\n",
       "       -2.270639e+01, -2.620040e+01, -2.988564e+01, -3.376767e+01,\n",
       "       -3.785219e+01, -4.214504e+01, -4.665221e+01, -5.137986e+01,\n",
       "       -5.633429e+01, -6.152196e+01, -6.694949e+01, -7.262369e+01,\n",
       "       -7.855150e+01, -8.474004e+01, -9.119663e+01, -9.792873e+01,\n",
       "       -1.049440e+02, -1.122502e+02, -1.198554e+02, -1.277678e+02,\n",
       "       -1.359958e+02, -1.445479e+02, -1.534328e+02, -1.626596e+02,\n",
       "       -1.722374e+02, -1.821754e+02, -1.924831e+02, -2.031704e+02,\n",
       "       -2.142472e+02, -2.257234e+02, -2.376095e+02, -2.499158e+02,\n",
       "       -2.626532e+02, -2.758325e+02, -2.894648e+02, -3.035613e+02,\n",
       "       -3.181335e+02, -3.331931e+02, -3.487520e+02, -3.648220e+02,\n",
       "       -3.814154e+02, -3.985447e+02, -4.162223e+02, -4.344611e+02,\n",
       "       -4.532738e+02, -4.726735e+02, -4.926735e+02, -5.132870e+02,\n",
       "       -5.345276e+02, -5.564089e+02, -5.789446e+02, -6.021486e+02,\n",
       "       -6.260349e+02, -6.506176e+02, -6.759107e+02, -7.019286e+02,\n",
       "       -7.286856e+02, -7.561960e+02, -7.844743e+02, -8.135349e+02,\n",
       "       -8.433922e+02, -8.740607e+02, -9.055548e+02, -9.378891e+02,\n",
       "       -9.710779e+02, -1.005135e+03, -1.040076e+03, -1.075914e+03,\n",
       "       -1.112664e+03, -1.150338e+03, -1.188952e+03, -1.228519e+03,\n",
       "       -1.269052e+03, -1.310564e+03, -1.353069e+03, -1.396580e+03,\n",
       "       -1.441109e+03, -1.486668e+03, -1.533269e+03, -1.580925e+03,\n",
       "       -1.629647e+03, -1.679445e+03, -1.730330e+03, -1.782314e+03,\n",
       "       -1.835405e+03, -1.889613e+03, -1.944947e+03, -2.001417e+03,\n",
       "       -2.059029e+03, -2.117792e+03, -2.177714e+03, -2.238800e+03,\n",
       "       -2.301058e+03, -2.364492e+03, -2.429108e+03, -2.494910e+03,\n",
       "       -2.561903e+03, -2.630090e+03, -2.699474e+03, -2.770057e+03,\n",
       "       -2.841841e+03, -2.914827e+03, -2.989016e+03, -3.064407e+03,\n",
       "       -3.141001e+03, -3.218796e+03, -3.297790e+03, -3.377981e+03,\n",
       "       -3.459366e+03, -3.541942e+03, -3.625704e+03, -3.710647e+03,\n",
       "       -3.796768e+03, -3.884060e+03, -3.972516e+03, -4.062130e+03,\n",
       "       -4.152896e+03, -4.244804e+03, -4.337848e+03, -4.432018e+03,\n",
       "       -4.527304e+03, -4.623699e+03, -4.721191e+03, -4.819771e+03,\n",
       "       -4.919427e+03, -5.020149e+03, -5.121926e+03, -5.224745e+03,\n",
       "       -5.328594e+03, -5.433461e+03, -5.539333e+03, -5.646199e+03,\n",
       "       -5.754044e+03], dtype=float32)</pre></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>lat</span></div><div class='xr-var-dims'>(lat)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>30.1875 30.229166 ... 45.979168</div><input id='attrs-abc7db12-9217-4f7c-8ddf-d0060f9801f5' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-abc7db12-9217-4f7c-8ddf-d0060f9801f5' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-e7a0649f-e1cf-4d60-a994-27842673c4f8' class='xr-var-data-in' type='checkbox'><label for='data-e7a0649f-e1cf-4d60-a994-27842673c4f8' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>degrees_north</dd><dt><span>long_name :</span></dt><dd>latitude</dd><dt><span>standard_name :</span></dt><dd>latitude</dd><dt><span>axis :</span></dt><dd>Y</dd></dl></div><pre class='xr-var-data'>array([30.1875  , 30.229166, 30.270834, ..., 45.895832, 45.9375  , 45.979168],\n",
       "      dtype=float32)</pre></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>lon</span></div><div class='xr-var-dims'>(lon)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>-17.291666 -17.25 ... 36.291668</div><input id='attrs-f753e042-bb2f-4fe3-9e59-3a451689b667' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-f753e042-bb2f-4fe3-9e59-3a451689b667' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-2f1d1acc-b46b-49a1-8beb-03e9a3a801c9' class='xr-var-data-in' type='checkbox'><label for='data-2f1d1acc-b46b-49a1-8beb-03e9a3a801c9' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>degrees_east</dd><dt><span>long_name :</span></dt><dd>longitude</dd><dt><span>standard_name :</span></dt><dd>longitude</dd><dt><span>axis :</span></dt><dd>X</dd></dl></div><pre class='xr-var-data'>array([-17.291666, -17.25    , -17.208334, ...,  36.208332,  36.25    ,\n",
       "        36.291668], dtype=float32)</pre></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>2020-06-11</div><input id='attrs-70d3f417-84b1-40eb-8aaf-281d8a8bdf74' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-70d3f417-84b1-40eb-8aaf-281d8a8bdf74' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-1f84d4e5-1ef9-4373-8f75-4ec01635ad2d' class='xr-var-data-in' type='checkbox'><label for='data-1f84d4e5-1ef9-4373-8f75-4ec01635ad2d' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>time</dd><dt><span>standard_name :</span></dt><dd>time</dd><dt><span>axis :</span></dt><dd>T</dd></dl></div><pre class='xr-var-data'>array([&#x27;2020-06-11T00:00:00.000000000&#x27;], dtype=&#x27;datetime64[ns]&#x27;)</pre></li></ul></div></li><li class='xr-section-item'><input id='section-dd0661f4-be10-4bc7-985f-e750689f2478' class='xr-section-summary-in' type='checkbox'  checked><label for='section-dd0661f4-be10-4bc7-985f-e750689f2478' class='xr-section-summary' >Data variables: <span>(2)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>bottomT</span></div><div class='xr-var-dims'>(time, lat, lon)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-ab9559a9-1d83-47ad-af01-f07e7842491c' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-ab9559a9-1d83-47ad-af01-f07e7842491c' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-49b18eae-d740-43cd-951d-40dbe546d451' class='xr-var-data-in' type='checkbox'><label for='data-49b18eae-d740-43cd-951d-40dbe546d451' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>standard_name :</span></dt><dd>sea_water_potential_temperature_at_sea_floor</dd><dt><span>long_name :</span></dt><dd>bottom temperature</dd><dt><span>valid_min :</span></dt><dd>1.0</dd><dt><span>units :</span></dt><dd>degrees_C</dd><dt><span>valid_max :</span></dt><dd>35.0</dd></dl></div><pre class='xr-var-data'>[489060 values with dtype=float32]</pre></li><li class='xr-var-item'><div class='xr-var-name'><span>thetao</span></div><div class='xr-var-dims'>(time, depth, lat, lon)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-b019a7a9-bd96-40b5-b665-053a8dc2b992' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-b019a7a9-bd96-40b5-b665-053a8dc2b992' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-bfc02f0e-3427-443f-9a09-f504cf4983b1' class='xr-var-data-in' type='checkbox'><label for='data-bfc02f0e-3427-443f-9a09-f504cf4983b1' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>standard_name :</span></dt><dd>sea_water_potential_temperature</dd><dt><span>long_name :</span></dt><dd>temperature</dd><dt><span>valid_min :</span></dt><dd>1.0</dd><dt><span>units :</span></dt><dd>degrees_C</dd><dt><span>valid_max :</span></dt><dd>35.0</dd></dl></div><pre class='xr-var-data'>[68957460 values with dtype=float32]</pre></li></ul></div></li><li class='xr-section-item'><input id='section-512cf5d4-7dc5-4791-ba60-957efb800f74' class='xr-section-summary-in' type='checkbox'  ><label for='section-512cf5d4-7dc5-4791-ba60-957efb800f74' class='xr-section-summary' >Attributes: <span>(12)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>comment :</span></dt><dd>Please check in CMEMS catalogue the INFO section for product MEDSEA_ANALYSIS_FORECAST_PHY_006_013 - http://marine.copernicus.eu</dd><dt><span>field_type :</span></dt><dd>daily_mean_centered_at_time_field</dd><dt><span>title :</span></dt><dd>Potential Temperature (3D) - Daily Mean</dd><dt><span>Conventions :</span></dt><dd>CF-1.0</dd><dt><span>source :</span></dt><dd>MFS EAS4</dd><dt><span>contact :</span></dt><dd>servicedesk.cmems@mercator-ocean.eu</dd><dt><span>references :</span></dt><dd>Please check in CMEMS catalogue the INFO section for product MEDSEA_ANALYSIS_FORECAST_PHY_006_013 - http://marine.copernicus.eu</dd><dt><span>bulletin_type :</span></dt><dd>analysis</dd><dt><span>bulletin_date :</span></dt><dd>20200601</dd><dt><span>institution :</span></dt><dd>Centro Euro-Mediterraneo sui Cambiamenti Climatici - CMCC, Italy</dd><dt><span>history :</span></dt><dd>Tue Jun  2 00:48:52 2020: ncks -4 -L 1 /home/datawork-oco-intranet/cmcc_mfseas4/tmp/20200611_d-CMCC--TEMP-MFSeas4-MEDATL-b20200601_fc-sv05.00.nc /home/datawork-oco-intranet/cmcc_mfseas4/tmp/20200611_d-CMCC--TEMP-MFSeas4-MEDATL-b20200601_fc-sv05.00.ncTC</dd><dt><span>NCO :</span></dt><dd>4.0.3</dd></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (depth: 141, lat: 380, lon: 1287, time: 1)\n",
       "Coordinates:\n",
       "  * depth    (depth) float32 -1.0182366 -3.1657474 ... -5646.199 -5754.044\n",
       "  * lat      (lat) float32 30.1875 30.229166 30.270834 ... 45.9375 45.979168\n",
       "  * lon      (lon) float32 -17.291666 -17.25 -17.208334 ... 36.25 36.291668\n",
       "  * time     (time) datetime64[ns] 2020-06-11\n",
       "Data variables:\n",
       "    bottomT  (time, lat, lon) float32 ...\n",
       "    thetao   (time, depth, lat, lon) float32 ...\n",
       "Attributes:\n",
       "    comment:        Please check in CMEMS catalogue the INFO section for prod...\n",
       "    field_type:     daily_mean_centered_at_time_field\n",
       "    title:          Potential Temperature (3D) - Daily Mean\n",
       "    Conventions:    CF-1.0\n",
       "    source:         MFS EAS4\n",
       "    contact:        servicedesk.cmems@mercator-ocean.eu\n",
       "    references:     Please check in CMEMS catalogue the INFO section for prod...\n",
       "    bulletin_type:  analysis\n",
       "    bulletin_date:  20200601\n",
       "    institution:    Centro Euro-Mediterraneo sui Cambiamenti Climatici - CMCC...\n",
       "    history:        Tue Jun  2 00:48:52 2020: ncks -4 -L 1 /home/datawork-oco...\n",
       "    NCO:            4.0.3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change depth sign\n",
    "ds.assign_coords(depth = z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instantiate a PCM with 8 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pcm 'gmm' (K: 8, F: 1)>\n",
       "Number of class: 8\n",
       "Number of feature: 1\n",
       "Feature names: odict_keys(['temperature'])\n",
       "Fitted: False\n",
       "Feature: 'temperature'\n",
       "\t Interpoler: <class 'pyxpcm.utils.Vertical_Interpolator'>\n",
       "\t Scaler: 'normal', <class 'sklearn.preprocessing._data.StandardScaler'>\n",
       "\t Reducer: True, <class 'sklearn.decomposition._pca.PCA'>\n",
       "Classifier: 'gmm', <class 'sklearn.mixture._gaussian_mixture.GaussianMixture'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = pcm(K=8, features=pcm_features)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model on data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tell the PCM model how to identify features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'temperature': 'thetao'}\n"
     ]
    }
   ],
   "source": [
    "features_in_ds = {'temperature': 'thetao'}\n",
    "print(features_in_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "specify what is the vertical dimension of the dataset variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_zdim='depth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit the model on the this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'depth' not found in array dimensions ('time', 'lat', 'lon')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/home1/datawork/kbalem/EnvConda/BlueCloud/lib/python3.6/site-packages/xarray/core/common.py\u001b[0m in \u001b[0;36m_get_axis_num\u001b[0;34m(self, dim)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: tuple.index(x): x not in tuple",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-fad7b5d80e48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures_in_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures_zdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home1/datawork/kbalem/EnvConda/BlueCloud/lib/python3.6/site-packages/pyxpcm/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, ds, features, dim)\u001b[0m\n\u001b[1;32m    859\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_args\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0;31m# PRE-PROCESSING:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m             \u001b[0;31m# CLASSIFICATION-MODEL TRAINING:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home1/datawork/kbalem/EnvConda/BlueCloud/lib/python3.6/site-packages/pyxpcm/models.py\u001b[0m in \u001b[0;36mpreprocessing\u001b[0;34m(self, ds, features, dim, action, mask)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_context\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.1-mask'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m                     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyxpcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                     \u001b[0;31m# Stack all-features mask:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'sampling'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home1/datawork/kbalem/EnvConda/BlueCloud/lib/python3.6/site-packages/pyxpcm/xarray.py\u001b[0m in \u001b[0;36mmask\u001b[0;34m(self, this_pcm, features, dim)\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0mz_ok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_inside\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_bto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_top\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0mNz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_ok\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_name_in_ds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mz_ok\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mNz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                 \u001b[0;31m# mask = self._obj[feature_name_in_ds]\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home1/datawork/kbalem/EnvConda/BlueCloud/lib/python3.6/site-packages/xarray/core/common.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(self, dim, axis, skipna, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home1/datawork/kbalem/EnvConda/BlueCloud/lib/python3.6/site-packages/xarray/core/dataarray.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, func, dim, axis, keep_attrs, keepdims, **kwargs)\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \"\"\"\n\u001b[1;32m   2245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2246\u001b[0;31m         \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_attrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2247\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replace_maybe_drop_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home1/datawork/kbalem/EnvConda/BlueCloud/lib/python3.6/site-packages/xarray/core/variable.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, func, dim, axis, keep_attrs, keepdims, allow_lazy, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_axis_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mallow_lazy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home1/datawork/kbalem/EnvConda/BlueCloud/lib/python3.6/site-packages/xarray/core/common.py\u001b[0m in \u001b[0;36mget_axis_num\u001b[0;34m(self, dim)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_axis_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mHashable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home1/datawork/kbalem/EnvConda/BlueCloud/lib/python3.6/site-packages/xarray/core/common.py\u001b[0m in \u001b[0;36m_get_axis_num\u001b[0;34m(self, dim)\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{dim!r} not found in array dimensions {self.dims!r}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'depth' not found in array dimensions ('time', 'lat', 'lon')"
     ]
    }
   ],
   "source": [
    "m.fit(ds, feature=features_in_ds, dim=features_zdim)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCM BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_in_ds = {'temperature': 'thetao'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This pcm instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-e63e64a1493c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures_in_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home1/datawork/kbalem/EnvConda/BlueCloud/lib/python3.6/site-packages/pyxpcm/models.py\u001b[0m in \u001b[0;36mbic\u001b[0;34m(self, ds, features, dim)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m             \u001b[0;31m# Check if the PCM is trained:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m             \u001b[0mvalidation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fitted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m             \u001b[0;31m# PRE-PROCESSING:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home1/datawork/kbalem/EnvConda/BlueCloud/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home1/datawork/kbalem/EnvConda/BlueCloud/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This pcm instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "m.bic(ds, features=features_in_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to arange data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'thetao'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-47ebf7b7b5d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtemperature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthetao\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'depth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'thetao'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home1/datawork/kbalem/EnvConda/BlueCloud/lib/python3.6/site-packages/pyxpcm/models.py\u001b[0m in \u001b[0;36mravel\u001b[0;34m(self, da, dim, feature_name)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# Is this a thick array or a slice ?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mis_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_props\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Load mask where all features are available for this PCM:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'thetao'"
     ]
    }
   ],
   "source": [
    "temperature = ds.thetao\n",
    "m.ravel(da = temperature, dim = 'depth', feature_name = 'thetao')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'thetao' (depth: 141, n_prof: 489060)>\n",
      "array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       ...,\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)\n",
      "Coordinates:\n",
      "  * depth    (depth) float32 1.0182366 3.1657474 5.4649634 ... 5646.199 5754.044\n",
      "    time     datetime64[ns] 2020-06-11\n",
      "  * n_prof   (n_prof) MultiIndex\n",
      "  - lat      (n_prof) float64 30.19 30.19 30.19 30.19 ... 45.98 45.98 45.98\n",
      "  - lon      (n_prof) float64 -17.29 -17.25 -17.21 -17.17 ... 36.21 36.25 36.29\n",
      "Attributes:\n",
      "    standard_name:  sea_water_potential_temperature\n",
      "    long_name:      temperature\n",
      "    valid_min:      1.0\n",
      "    units:          degrees_C\n",
      "    valid_max:      35.0\n"
     ]
    }
   ],
   "source": [
    "ds_new = ds.drop_vars('bottomT')\n",
    "ds_new = ds_new.stack(n_prof=('lat', 'lon'))\n",
    "ds_new = ds_new.squeeze()\n",
    "print(ds_new.thetao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fit the model on data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tell the PCM model how to identify features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'temperature': 'thetao'}\n"
     ]
    }
   ],
   "source": [
    "features_in_ds = {'temperature': 'thetao'}\n",
    "print(features_in_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "specify what is the vertical dimension of the dataset variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_zdim='depth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit the model on the this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'depth' not found in array dimensions ('n_prof',)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/home1/datawork/kbalem/EnvConda/BlueCloud/lib/python3.6/site-packages/xarray/core/common.py\u001b[0m in \u001b[0;36m_get_axis_num\u001b[0;34m(self, dim)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: tuple.index(x): x not in tuple",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-54de846438a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures_in_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures_zdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# m.fit(ds_new, features=features_in_ds, dim=features_zdim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home1/datawork/kbalem/EnvConda/BlueCloud/lib/python3.6/site-packages/pyxpcm/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, ds, features, dim)\u001b[0m\n\u001b[1;32m    859\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_args\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0;31m# PRE-PROCESSING:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m             \u001b[0;31m# CLASSIFICATION-MODEL TRAINING:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home1/datawork/kbalem/EnvConda/BlueCloud/lib/python3.6/site-packages/pyxpcm/models.py\u001b[0m in \u001b[0;36mpreprocessing\u001b[0;34m(self, ds, features, dim, action, mask)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_context\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.1-mask'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m                     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyxpcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                     \u001b[0;31m# Stack all-features mask:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'sampling'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home1/datawork/kbalem/EnvConda/BlueCloud/lib/python3.6/site-packages/pyxpcm/xarray.py\u001b[0m in \u001b[0;36mmask\u001b[0;34m(self, this_pcm, features, dim)\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0mz_ok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_inside\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_bto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_top\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0mNz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_ok\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_name_in_ds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mz_ok\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mNz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                 \u001b[0;31m# mask = self._obj[feature_name_in_ds]\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home1/datawork/kbalem/EnvConda/BlueCloud/lib/python3.6/site-packages/xarray/core/common.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(self, dim, axis, skipna, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home1/datawork/kbalem/EnvConda/BlueCloud/lib/python3.6/site-packages/xarray/core/dataarray.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, func, dim, axis, keep_attrs, keepdims, **kwargs)\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \"\"\"\n\u001b[1;32m   2245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2246\u001b[0;31m         \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_attrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2247\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replace_maybe_drop_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home1/datawork/kbalem/EnvConda/BlueCloud/lib/python3.6/site-packages/xarray/core/variable.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, func, dim, axis, keep_attrs, keepdims, allow_lazy, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_axis_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mallow_lazy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home1/datawork/kbalem/EnvConda/BlueCloud/lib/python3.6/site-packages/xarray/core/common.py\u001b[0m in \u001b[0;36mget_axis_num\u001b[0;34m(self, dim)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_axis_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mHashable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home1/datawork/kbalem/EnvConda/BlueCloud/lib/python3.6/site-packages/xarray/core/common.py\u001b[0m in \u001b[0;36m_get_axis_num\u001b[0;34m(self, dim)\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{dim!r} not found in array dimensions {self.dims!r}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'depth' not found in array dimensions ('n_prof',)"
     ]
    }
   ],
   "source": [
    "m.fit(ds_new, features=features_in_ds, dim=features_zdim)\n",
    "# m.fit(ds_new, features=features_in_ds, dim=features_zdim)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load /home1/datawork/kbalem/EnvConda/BlueCloud/lib/python3.6/site-packages/pyxpcm/models.py\n",
    "\"\"\"\n",
    "\n",
    ".. module:: pyxpcm\n",
    "   :synopsis: Profile Classification Model\n",
    "\n",
    ".. moduleauthor:: Guillaume Maze <gmaze@ifremer.fr>\n",
    "\n",
    "Multi-variables classification, ie use of more than physical variable as PCM features\n",
    "\n",
    "Created on 2019/09/27\n",
    "@author: G. Maze (Ifremer/LOPS)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import collections\n",
    "import inspect\n",
    "import dask\n",
    "\n",
    "import warnings\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from datetime import datetime\n",
    "\n",
    "# Internal:\n",
    "from .plot import _PlotMethods\n",
    "from .stat import _StatMethods\n",
    "from .utils import LogDataType, Vertical_Interpolator, NoTransform, StatisticsBackend, docstring\n",
    "from . import io\n",
    "\n",
    "# Scikit-learn useful methods:\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import validation\n",
    "from sklearn.utils import assert_all_finite\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "####### Scikit-learn statistic backend:\n",
    "# http://scikit-learn.org/stable/modules/mixture.html\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "class PCMFeatureError(Exception):\n",
    "    \"\"\"Exception raised when features not correct\"\"\"\n",
    "\n",
    "class PCMClassError(Exception):\n",
    "    \"\"\"Exception raised when classes not correct\"\"\"\n",
    "\n",
    "class pcm(object):\n",
    "    \"\"\"Profile Classification Model class constructor\n",
    "\n",
    "        Consume and return :mod:`xarray` objects\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 K:int,\n",
    "                 features:dict(),\n",
    "                 scaling=1,\n",
    "                 reduction=1, maxvar=15,\n",
    "                 classif='gmm', covariance_type='full',\n",
    "                 verb=False,\n",
    "                 debug=False,\n",
    "                 timeit=False, timeit_verb=False,\n",
    "                 chunk_size='auto',\n",
    "                 backend='sklearn'):\n",
    "        \"\"\"Create the PCM instance\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        K: int\n",
    "            The number of class, or cluster, in the classification model.\n",
    "\n",
    "        features: dict()\n",
    "            The vertical axis to use for each features.\n",
    "            eg: {'temperature':np.arange(-2000,0,1)}\n",
    "\n",
    "        scaling: int (default: 1)\n",
    "            Define the scaling method:\n",
    "\n",
    "            - 0: No scaling\n",
    "            - **1: Center on sample mean and scale by sample std**\n",
    "            - 2: Center on sample mean only\n",
    "\n",
    "        reduction: int (default: 1)\n",
    "            Define the dimensionality reduction method:\n",
    "\n",
    "            - 0: No reduction\n",
    "            - **1: Reduction using :class:`sklearn.decomposition.PCA`**\n",
    "\n",
    "        maxvar: float (default: 99.9)\n",
    "            Maximum feature variance to preserve in the reduced dataset using :class:`sklearn.decomposition.PCA`. In %.\n",
    "\n",
    "        classif: str (default: 'gmm')\n",
    "            Define the classification method.\n",
    "            The only method available as of now is a Gaussian Mixture Model.\n",
    "            See :class:`sklearn.mixture.GaussianMixture` for more details.\n",
    "\n",
    "        covariance_type: str (default: 'full')\n",
    "            Define the type of covariance matrix shape to be used in the default classifier GMM.\n",
    "            It can be ‘full’ (default), ‘tied’, ‘diag’ or ‘spherical’.\n",
    "\n",
    "        verb: boolean (default: False)\n",
    "            More verbose output\n",
    "\n",
    "        timeit: boolean (default: False)\n",
    "            Register time of operation for performance evaluation\n",
    "\n",
    "        timeit_verb: boolean (default: False)\n",
    "            Print time of operation during execution\n",
    "\n",
    "        chunk_size: 'auto' or int\n",
    "            Sampling chunk size, (array of features after pre-processing)\n",
    "\n",
    "        backend: str\n",
    "            Statistic library backend, 'sklearn' (default) or 'dask_ml'\n",
    "\n",
    "        \"\"\"\n",
    "        if K==0:\n",
    "            raise PCMClassError(\"Can't create a PCM with K=0\")\n",
    "        if K is None:\n",
    "            raise PCMClassError(\"K must be defined to create a PMC\")\n",
    "        if not bool(features):\n",
    "            raise PCMFeatureError(\"Can't create a PCM without features\")\n",
    "\n",
    "        if   scaling==0: with_scaler = 'none'; with_mean=False; with_std = False\n",
    "        elif scaling==1: with_scaler = 'normal'; with_mean=True; with_std = True\n",
    "        elif scaling==2: with_scaler = 'center'; with_mean=True; with_std = False\n",
    "        else: raise NameError('scaling must be 0, 1 or 2')\n",
    "        \n",
    "        if   reduction==0: with_reducer = False\n",
    "        elif reduction==1: with_reducer = True\n",
    "        else: raise NameError('reduction must be 0 or 1')\n",
    "        \n",
    "        if classif=='gmm': with_classifier = 'gmm';\n",
    "        else: raise NameError(\"classifier must be 'gmm' (no other methods implemented at this time)\")\n",
    "\n",
    "        #todo check validity of the dict of features\n",
    "\n",
    "        self._props = {'K': np.int(K),\n",
    "                       'F': len(features),\n",
    "                        'llh': None,\n",
    "                        'COVARTYPE': covariance_type,\n",
    "                        'with_scaler': with_scaler,\n",
    "                        'with_reducer': with_reducer,\n",
    "                        'with_classifier': with_classifier,\n",
    "                        'maxvar': maxvar,\n",
    "                        'features': collections.OrderedDict(features),\n",
    "                        'chunk_size': chunk_size,\n",
    "                        'backend': backend,\n",
    "                        'cmap': None}\n",
    "        self._xmask = None # xarray mask for nd-array used at pre-processing steps\n",
    "\n",
    "        self._verb = verb #todo _verb is a property, should be set/get with a decorator\n",
    "        self._debug = debug\n",
    "\n",
    "        self._interpoler = collections.OrderedDict()\n",
    "        self._scaler = collections.OrderedDict()\n",
    "        self._scaler_props = collections.OrderedDict()\n",
    "        self._reducer = collections.OrderedDict()\n",
    "        self._homogeniser = collections.OrderedDict()\n",
    "\n",
    "        # Load estimators for a specific backend:\n",
    "        bck = StatisticsBackend(backend, scaler='StandardScaler', reducer='PCA')\n",
    "\n",
    "        for feature_name in features:\n",
    "            feature_axis = self._props['features'][feature_name]\n",
    "            if isinstance(feature_axis, xr.DataArray):\n",
    "                self._props['features'][feature_name] = feature_axis.values\n",
    "\n",
    "            # self._scaler[feature_name] = preprocessing.StandardScaler(with_mean=with_mean,\n",
    "            #                                             with_std=with_std)\n",
    "            if 'none' not in self._props['with_scaler']:\n",
    "                self._scaler[feature_name] = bck.scaler(with_mean=with_mean, with_std=with_std)\n",
    "            else:\n",
    "                self._scaler[feature_name] = NoTransform()\n",
    "            self._scaler_props[feature_name] = {'units': '?'}\n",
    "\n",
    "            is_slice = np.all(feature_axis == None)\n",
    "            if not is_slice:\n",
    "                self._interpoler[feature_name] = Vertical_Interpolator(axis=feature_axis, debug=self._debug)\n",
    "                if np.prod(feature_axis.shape) == 1:\n",
    "                    # Single level: no need to reduce\n",
    "                    if self._debug: print('Single level, not need to reduce', np.prod(feature_axis.ndim))\n",
    "                    self._reducer[feature_name] = NoTransform()\n",
    "                else:\n",
    "                    # Multi-vertical-levels, set reducer:\n",
    "                    if with_reducer:\n",
    "                        self._reducer[feature_name] = bck.reducer(n_components=self._props['maxvar'],\n",
    "                                                                  svd_solver='full')\n",
    "                    else:\n",
    "                        self._reducer[feature_name] = NoTransform()\n",
    "            else:\n",
    "                self._interpoler[feature_name] = NoTransform()\n",
    "                self._reducer[feature_name] = NoTransform()\n",
    "                if self._debug: print(\"%s is single level, no need to reduce\" % feature_name)\n",
    "\n",
    "            self._homogeniser[feature_name] = {'mean': 0, 'std': 1}\n",
    "\n",
    "        self._classifier = GaussianMixture(n_components=self._props['K'],\n",
    "                                          covariance_type=self._props['COVARTYPE'],\n",
    "                                          init_params='kmeans',\n",
    "                                          max_iter=1000,\n",
    "                                          tol=1e-6)\n",
    "\n",
    "        # Define the \"context\" to execute some functions inner code\n",
    "        # (useful for time benchmarking)\n",
    "        self._context = self.__empty_context # Default is empty, do nothing\n",
    "        self._context_args = dict()\n",
    "        if timeit:\n",
    "            self._context = self.__timeit_context\n",
    "            self._context_args = {'maxlevel': 3, 'verb':timeit_verb}\n",
    "            self._timeit = dict()\n",
    "\n",
    "        # Define statistics for the fit method:\n",
    "        self._fit_stats = dict({'datetime': None, 'n_samples_seen_': None, 'score': None, 'etime': None})\n",
    "\n",
    "    @contextmanager\n",
    "    def __timeit_context(self, name, opts=dict()):\n",
    "        default_opts = {'maxlevel': np.inf, 'verb':False}\n",
    "        for key in opts:\n",
    "            if key in default_opts:\n",
    "                default_opts[key] = opts[key]\n",
    "        level = len([i for i in range(len(name)) if name.startswith('.', i)])\n",
    "        if level <= default_opts['maxlevel']:\n",
    "            startTime = time.time()\n",
    "            yield\n",
    "            elapsedTime = time.time() - startTime\n",
    "            trailingspace = \" \" * level\n",
    "            trailingspace = \" \"\n",
    "            if default_opts['verb']:\n",
    "                # print('... time in {} {}: {} ms'.format(trailingspace, name, int(elapsedTime * 1000)))\n",
    "                print('{} {}: {} ms'.format(trailingspace, name, int(elapsedTime * 1000)))\n",
    "            if name in self._timeit:\n",
    "                self._timeit[name].append(elapsedTime * 1000)\n",
    "            else:\n",
    "                self._timeit[name] = list([elapsedTime*1000])\n",
    "        else:\n",
    "            yield\n",
    "\n",
    "    @contextmanager\n",
    "    def __empty_context(self, name, *args, **kargs):\n",
    "        yield\n",
    "\n",
    "    def __call__(self, **kwargs):\n",
    "        self.__init__(**kwargs)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.__i = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.__i < self.K:\n",
    "            i = self.__i\n",
    "            self.__i += 1\n",
    "            return i\n",
    "        else:\n",
    "            raise StopIteration()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.display(deep=self._verb)\n",
    "\n",
    "    # def xmerge(self, ds, da):\n",
    "    #     \"\"\" Add a new :class:`xarray.DataArray` to a :class:`xarray.Dataset` \"\"\"\n",
    "    #\n",
    "    #     if da.name in ds.data_vars:\n",
    "    #         warnings.warn((\"%s variable already in the dataset: overwriting\") % (da.name))\n",
    "    #\n",
    "    #     # Add pyXpcm tracking clues:\n",
    "    #     da.attrs['comment'] = \"Automatically added by pyXpcm\"\n",
    "    #\n",
    "    #     #\n",
    "    #     # vname = da.name\n",
    "    #     # self._obj[da.name] = da\n",
    "    #     ds = xr.merge([ds, da])\n",
    "    #     return ds\n",
    "    #\n",
    "    # def __clean(self, ds):\n",
    "    #     \"\"\" Remove all variables created with pyXpcm front a :class:`xarray.Dataset` \"\"\"\n",
    "    #     # See add() method to identify these variables.\n",
    "    #     for vname in ds.data_vars:\n",
    "    #         if (\"comment\" in ds[vname].attrs) \\\n",
    "    #             and (ds[vname].attrs['comment'] == \"Automatically added by pyXpcm\"):\n",
    "    #             ds = ds.drop(vname)\n",
    "    #     return ds\n",
    "\n",
    "    def ravel(self, da, dim=None, feature_name=str):\n",
    "        \"\"\" Extract from N-d array a X(feature,sample) 2-d array and vertical dimension z\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            da: :class:`xarray.DataArray`\n",
    "                The DataArray to process\n",
    "\n",
    "            dim: str\n",
    "                Name of the vertical dimension in the input :class:`xarray.DataArray`\n",
    "\n",
    "            feature_name: str\n",
    "                Target PCM feature name for the input :class:`xarray.DataArray`\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            X: :class:`xarray.DataArray`\n",
    "                A new DataArray with dimension ['n_sampling','n_features']\n",
    "                Note that data are always :class:`dask.array.Array`.\n",
    "\n",
    "            z: :class:`numpy.array`\n",
    "                The vertical axis of data\n",
    "\n",
    "            sampling_dims: dict()\n",
    "                Dictionary where keys are :class:`xarray.Dataset` variable names of features\n",
    "                and values are another dictionary with the list of sampling dimension in\n",
    "                ``DIM_SAMPLING`` key and the name of the vertical axis in the ``DIM_VERTICAL`` key.\n",
    "\n",
    "            Examples\n",
    "            --------\n",
    "            This function is meant to be used internally only\n",
    "\n",
    "            __author__: gmaze@ifremer.fr\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Is this a thick array or a slice ?\n",
    "        is_slice = np.all(self._props['features'][feature_name] == None)\n",
    "\n",
    "        # Load mask where all features are available for this PCM:\n",
    "        mask_stacked = self._xmask\n",
    "\n",
    "        if is_slice:\n",
    "            # No vertical dimension to use, simple stacking\n",
    "            sampling_dims = list(da.dims)\n",
    "            # Apply all-features mask:\n",
    "            X = da.stack({'sampling': sampling_dims})\n",
    "            X = X.where(mask_stacked == 1, drop=True).expand_dims('dummy').transpose()#.values\n",
    "            z = np.empty((1,))\n",
    "        else:\n",
    "            if not dim:\n",
    "                # Try to infer the vertical dimension name looking for the CF 'axis' attribute in all dimensions\n",
    "                dim_found = False\n",
    "                for this_dim in da.dims:\n",
    "                    if ('axis' in da[this_dim].attrs) and (da[this_dim].attrs['axis'] == 'Z'):\n",
    "                        dim = this_dim\n",
    "                        dim_found = True\n",
    "                if not dim_found:\n",
    "                    raise PCMFeatureError(\"You must specify a vertical dimension name: \"\\\n",
    "                                          \"use argument 'dim' or \"\\\n",
    "                                          \"specify DataSet dimension the attribute 'axis' to 'Z' (CF1.6)\")\n",
    "            elif dim not in da.dims:\n",
    "                raise ValueError(\"Vertical dimension %s not found in this DataArray\" % dim)\n",
    "\n",
    "            sampling_dims = list(da.dims)\n",
    "            sampling_dims.remove(dim)\n",
    "            X = da.stack({'sampling': sampling_dims}) #todo Improve performance for this operation !\n",
    "            # Apply all-features mask:\n",
    "            X = X.where(mask_stacked == 1, drop=True).transpose()\n",
    "            z = da[dim].values\n",
    "\n",
    "        X = X.chunk(chunks={'sampling': self._props['chunk_size']})\n",
    "        return X, z, sampling_dims\n",
    "\n",
    "    def unravel(self, ds, sampling_dims, X):\n",
    "        \"\"\" Create a DataArray from a numpy array and sampling dimensions \"\"\"\n",
    "\n",
    "        # Load mask where all features are available for this PCM:\n",
    "        mask_stacked = self._xmask\n",
    "\n",
    "        #\n",
    "        coords = list()\n",
    "        size = list()\n",
    "        for dim in sampling_dims:\n",
    "            coords.append(ds[dim])\n",
    "            size.append(len(ds[dim]))\n",
    "        da = xr.DataArray(np.empty((size)), coords=coords)\n",
    "        da = da.stack({'sampling': sampling_dims})\n",
    "        da = da.where(mask_stacked == 1, drop=True).transpose()\n",
    "        da.values = X\n",
    "        da = da.unstack('sampling')\n",
    "\n",
    "        if (np.prod(da.shape) != mask_stacked.shape[0]):\n",
    "            if self._debug:\n",
    "                print(\"\\tUnravelled data not matching mask dimension, re-indexing\")\n",
    "            mask = mask_stacked.unstack()\n",
    "            da = da.reindex_like(mask)\n",
    "\n",
    "        return da\n",
    "\n",
    "    @property\n",
    "    def K(self):\n",
    "        \"\"\"Return the number of classes\"\"\"\n",
    "        return self._props['K']\n",
    "\n",
    "    @property\n",
    "    def F(self):\n",
    "        \"\"\"Return the number of features\"\"\"\n",
    "        return self._props['F']\n",
    "\n",
    "    @property\n",
    "    def features(self):\n",
    "        \"\"\"Return features definition dictionnary\"\"\"\n",
    "        return self._props['features']\n",
    "\n",
    "    @property\n",
    "    def plot(self):\n",
    "        \"\"\"Access plotting functions\"\"\"\n",
    "        self._plt = _PlotMethods(self)\n",
    "        return self._plt\n",
    "\n",
    "    @property\n",
    "    def stat(self):\n",
    "        \"\"\"Access statistics functions\"\"\"\n",
    "        return _StatMethods(self)\n",
    "\n",
    "    @property\n",
    "    def timeit(self):\n",
    "        \"\"\" Return a :class:`pandas.DataFrame` with Execution time of method called on this instance \"\"\"\n",
    "\n",
    "        def get_multindex(times):\n",
    "            \"\"\" Create multi-index pandas \"\"\"\n",
    "            # Get max levels:\n",
    "            dpt = list()\n",
    "            [dpt.append(len(key.split(\".\"))) for key in times]\n",
    "            max_dpt = np.max(dpt)\n",
    "            # Read index:\n",
    "            levels_1 = list()\n",
    "            levels_2 = list()\n",
    "            levels_3 = list()\n",
    "            levels_4 = list()\n",
    "            if max_dpt == 1:\n",
    "                for key in times:\n",
    "                    levels = key.split(\".\")\n",
    "                    levels_1.append(levels[0])\n",
    "                return max_dpt, [levels_1]\n",
    "            elif max_dpt == 2:\n",
    "                for key in times:\n",
    "                    levels = key.split(\".\")\n",
    "                    if len(levels) == 1:\n",
    "                        levels_1.append(levels[0])\n",
    "                        levels_2.append('total')\n",
    "                    if len(levels) == 2:\n",
    "                        levels_1.append(levels[0])\n",
    "                        levels_2.append(levels[1])\n",
    "                return max_dpt, [levels_1,levels_2]\n",
    "            elif max_dpt == 3:\n",
    "                for key in times:\n",
    "                    levels = key.split(\".\")\n",
    "        #             print(len(levels), levels)\n",
    "                    if len(levels) == 1:\n",
    "                        levels_1.append(levels[0])\n",
    "                        levels_2.append('total')\n",
    "                        levels_3.append('')\n",
    "                    if len(levels) == 2:\n",
    "                        levels_1.append(levels[0])\n",
    "                        levels_2.append(levels[1])\n",
    "                        levels_3.append('total')\n",
    "                    if len(levels) == 3:\n",
    "                        levels_1.append(levels[0])\n",
    "                        levels_2.append(levels[1])\n",
    "                        levels_3.append(levels[2])\n",
    "                return max_dpt, [levels_1,levels_2,levels_3]\n",
    "            elif max_dpt == 4:\n",
    "                for key in times:\n",
    "                    levels = key.split(\".\")\n",
    "                    if len(levels) == 1:\n",
    "                        levels_1.append(levels[0])\n",
    "                        levels_2.append('total')\n",
    "                        levels_3.append('')\n",
    "                        levels_4.append('')\n",
    "                    if len(levels) == 2:\n",
    "                        levels_1.append(levels[0])\n",
    "                        levels_2.append(levels[1])\n",
    "                        levels_3.append('total')\n",
    "                        levels_4.append('')\n",
    "                    if len(levels) == 3:\n",
    "                        levels_1.append(levels[0])\n",
    "                        levels_2.append(levels[1])\n",
    "                        levels_3.append(levels[2])\n",
    "                        levels_4.append('total')\n",
    "                    if len(levels) == 4:\n",
    "                        levels_1.append(levels[0])\n",
    "                        levels_2.append(levels[1])\n",
    "                        levels_3.append(levels[2])\n",
    "                        levels_4.append(levels[3])\n",
    "                return max_dpt, [levels_1,levels_2,levels_3,levels_4]\n",
    "\n",
    "        times = self._timeit\n",
    "        max_dpt, arrays = get_multindex(times)\n",
    "        if max_dpt == 1:\n",
    "            index = pd.Index(arrays[0], names=['Method'])\n",
    "            df = pd.Series([np.sum(times[key]) for key in times], index=index)\n",
    "            # df = df.T\n",
    "        elif max_dpt == 2:\n",
    "            tuples = list(zip(*arrays))\n",
    "            index = pd.MultiIndex.from_tuples(tuples, names=['Method', 'Sub-method'])\n",
    "            df = pd.Series([np.sum(times[key]) for key in times], index=index)\n",
    "            df = df.unstack(0)\n",
    "            df = df.drop('total')\n",
    "            df = df.T\n",
    "        elif max_dpt == 3:\n",
    "            tuples = list(zip(*arrays))\n",
    "            index = pd.MultiIndex.from_tuples(tuples, names=['Method', 'Sub-method', 'Sub-sub-method'])\n",
    "            df = pd.Series([np.sum(times[key]) for key in times], index=index)\n",
    "    #         df = df.unstack(0)\n",
    "        elif max_dpt == 4:\n",
    "            tuples = list(zip(*arrays))\n",
    "            index = pd.MultiIndex.from_tuples(tuples, names=['Method', 'Sub-method', 'Sub-sub-method',\n",
    "                                                             'Sub-sub-sub-method'])\n",
    "            df = pd.Series([np.sum(times[key]) for key in times], index=index)\n",
    "\n",
    "        return df\n",
    "\n",
    "    @property\n",
    "    def backend(self):\n",
    "        \"\"\"Return the name of the statistic backend\"\"\"\n",
    "        return self._props['backend']\n",
    "\n",
    "    @property\n",
    "    def fitstats(self):\n",
    "        \"\"\" Estimator fit properties\n",
    "\n",
    "            The number of samples processed by the estimator\n",
    "            Will be reset on new calls to fit, but increments across partial_fit calls.\n",
    "        \"\"\"\n",
    "        return self._fit_stats\n",
    "\n",
    "    @docstring(io.to_netcdf.__doc__)\n",
    "    def to_netcdf(self, ncfile, **ka):\n",
    "        \"\"\" Save PCM to netcdf file\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            path : str\n",
    "                Path to file\n",
    "        \"\"\"\n",
    "        return io.to_netcdf(self, ncfile, **ka)\n",
    "\n",
    "    def display(self, deep=False):\n",
    "        \"\"\"Display detailed parameters of the PCM\n",
    "            This is not a get_params because it doesn't return a dictionary\n",
    "            Set Boolean option 'deep' to True for all properties display\n",
    "        \"\"\"\n",
    "        summary = [(\"<pcm '%s' (K: %i, F: %i)>\")%(self._props['with_classifier'],\n",
    "                                                  self._props['K'],\n",
    "                                                  len(self._props['features']))]\n",
    "        \n",
    "        # PCM core properties:\n",
    "        prop_info = ('Number of class: %i') % self._props['K']\n",
    "        summary.append(prop_info)\n",
    "        prop_info = ('Number of feature: %i') % len(self._props['features'])\n",
    "        summary.append(prop_info)\n",
    "\n",
    "        prop_info = ('Feature names: %s') % (repr(self._props['features'].keys()))\n",
    "        summary.append(prop_info)\n",
    "\n",
    "        # prop_info = ('Feature axis: [%s, ..., %s]') % (repr(self._props['features'][0]),\n",
    "        #                                                repr(self._props['feature_axis'][-1]))\n",
    "        # summary.append(prop_info)\n",
    "        \n",
    "        prop_info = ('Fitted: %r') % hasattr(self, 'fitted')\n",
    "        summary.append(prop_info)\n",
    "\n",
    "        # PCM workflow parameters:\n",
    "        for feature in self._props['features']:\n",
    "            prop_info = \"Feature: '%s'\" % feature\n",
    "            summary.append(prop_info)\n",
    "            summary.append(\"\\t Interpoler: %s\"%(type(self._interpoler[feature])))\n",
    "\n",
    "            # prop_info = ('\\t Sample Scaling: %r') %\n",
    "            # summary.append(prop_info)\n",
    "            summary.append(\"\\t Scaler: %r, %s\"%(self._props['with_scaler'], type(self._scaler[feature])))\n",
    "\n",
    "            if (deep):\n",
    "                # summary.append(\"\\t\\t Scaler properties:\")\n",
    "                d = self._scaler[feature].get_params(deep=deep)\n",
    "                for p in d: summary.append((\"\\t\\t %s: %r\")%(p,d[p]))\n",
    "\n",
    "            # prop_info = ('\\t Dimensionality Reduction: %r') %\n",
    "            # summary.append(prop_info)\n",
    "            summary.append(\"\\t Reducer: %r, %s\"%(self._props['with_reducer'], type(self._reducer[feature])))\n",
    "\n",
    "            if (deep):\n",
    "                # summary.append(\"\\t\\t Reducer properties:\")\n",
    "                d = self._reducer[feature].get_params(deep=deep)\n",
    "                for p in d: summary.append((\"\\t\\t %s: %r\")%(p,d[p]))\n",
    "        # return '\\n'.join(summary)\n",
    "\n",
    "        # prop_info = ('Classification: %r') %\n",
    "        # summary.append(prop_info)\n",
    "        summary.append(\"Classifier: %r, %s\"%(self._props['with_classifier'], type(self._classifier)))\n",
    "        #prop_info = ('GMM covariance type: %s') % self._props['COVARTYPE']\n",
    "        #summary.append(prop_info)\n",
    "        if (hasattr(self,'fitted')):\n",
    "            prop_info = ('\\t log likelihood of the training set: %f') % self._props['llh']\n",
    "            summary.append(prop_info)\n",
    "        \n",
    "        if (deep):\n",
    "            summary.append(\"\\t Classifier properties:\")\n",
    "            d = self._classifier.get_params(deep=deep)\n",
    "            for p in d: summary.append((\"\\t\\t %s: %r\")%(p,d[p]))\n",
    "        \n",
    "        # Done\n",
    "        return '\\n'.join(summary)\n",
    "\n",
    "    def preprocessing_this(self, da, dim=None, feature_name=str(), action='?'):\n",
    "        \"\"\"Pre-process data before anything\n",
    "\n",
    "        Possible pre-processing steps:\n",
    "\n",
    "        - interpolation,\n",
    "        - scaling,\n",
    "        - reduction\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        da: :class:`xarray.DataArray`\n",
    "            The DataArray to process\n",
    "\n",
    "        dim: str\n",
    "            Name of the vertical dimension in the input :class:`xarray.DataArray`\n",
    "\n",
    "        feature_name: str\n",
    "            Target PCM feature name for the input :class:`xarray.DataArray`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X: np.array\n",
    "            Pre-processed feature, with dimensions (N_SAMPLE, N_FEATURES)\n",
    "\n",
    "        sampling_dims: list()\n",
    "            List of the input :class:`xarray.DataArray` dimensions stacked as sampling points\n",
    "\n",
    "        \"\"\"\n",
    "        this_context = str(action)+'.1-preprocess.2-feature_'+feature_name\n",
    "        with self._context(this_context + '.total', self._context_args):\n",
    "\n",
    "            # MAKE THE ND-ARRAY A 2D-ARRAY\n",
    "            with self._context(this_context + '.1-ravel', self._context_args):\n",
    "                X, z, sampling_dims = self.ravel(da, dim=dim, feature_name=feature_name)\n",
    "                if self._debug:\n",
    "                    print(\"\\t\", \"X RAVELED with success\", str(LogDataType(X)))\n",
    "\n",
    "            # INTERPOLATION STEP:\n",
    "            with self._context(this_context + '.2-interp', self._context_args):\n",
    "                X = self._interpoler[feature_name].transform(X, z)\n",
    "                if self._debug:\n",
    "                    if isinstance(self._interpoler[feature_name], NoTransform):\n",
    "                        print(\"\\t\", \"X INTERPOLATED with success (NoTransform)\", str(LogDataType(X)))\n",
    "                    else:\n",
    "                        print(\"\\t\", \"X INTERPOLATED with success\", str(LogDataType(X)))\n",
    "                    # print(X.values.flags['WRITEABLE'])\n",
    "                    # After the interpolation step, we must not have nan in the 2d array:\n",
    "                    assert_all_finite(X, allow_nan=False)\n",
    "\n",
    "            # FIT STEPS:\n",
    "            # We need to fit pre-processing methods in order to re-use them when\n",
    "            # predicting a new dataset\n",
    "\n",
    "            # SCALING:\n",
    "            with self._context(this_context+'.3-scale_fit', self._context_args):\n",
    "                if not hasattr(self, 'fitted'):\n",
    "                    self._scaler[feature_name].fit(X.data)\n",
    "                    if 'units' in da.attrs:\n",
    "                        self._scaler_props[feature_name]['units'] = da.attrs['units']\n",
    "\n",
    "            with self._context(this_context + '.4-scale_transform', self._context_args):\n",
    "                try:\n",
    "                    X.data = self._scaler[feature_name].transform(X.data, copy=False)\n",
    "                except ValueError:\n",
    "                    if self._debug: print(\"\\t\\t Fail to scale.transform without copy, fall back on copy=True\")\n",
    "                    try:\n",
    "                        X.data = self._scaler[feature_name].transform(X.data, copy=True)\n",
    "                    except ValueError:\n",
    "                        if self._debug: print(\"\\t\\t Fail to scale.transform with copy, fall back on input copy\")\n",
    "                        X.data = self._scaler[feature_name].transform(X.data.copy())\n",
    "                        pass\n",
    "                    except:\n",
    "                        if self._debug: print(X.values.flags['WRITEABLE'])\n",
    "                        raise\n",
    "                    pass\n",
    "                except:\n",
    "                    raise\n",
    "\n",
    "                if self._debug:\n",
    "                    print(\"\\t\", \"X SCALED with success)\", str(LogDataType(X)))\n",
    "\n",
    "            # REDUCTION:\n",
    "            with self._context(this_context + '.5-reduce_fit', self._context_args):\n",
    "                if (not hasattr(self, 'fitted')) and (self._props['with_reducer']):\n",
    "\n",
    "                    if self.backend == 'dask_ml':\n",
    "                        # We have to convert any type of data array into a Dask array because\n",
    "                        # dask_ml cannot handle anything else (!)\n",
    "                        #todo Raise an issue on dask_ml github to ask why is this choice made\n",
    "                        # Related issues:\n",
    "                        #   https://github.com/dask/dask-ml/issues/6\n",
    "                        #   https://github.com/dask/dask-ml/issues/541\n",
    "                        #   https://github.com/dask/dask-ml/issues/542\n",
    "                        X.data = dask.array.asarray(X.data, chunks=X.shape)\n",
    "\n",
    "                    if isinstance(X.data, dask.array.Array):\n",
    "                        self._reducer[feature_name].fit(X.data)\n",
    "                    else:\n",
    "                        self._reducer[feature_name].fit(X)\n",
    "\n",
    "            with self._context(this_context + '.6-reduce_transform', self._context_args):\n",
    "                X = self._reducer[feature_name].transform(X.data) # Reduction, return np.array\n",
    "\n",
    "                # After reduction the new array is [ sampling, reduced_dim ]\n",
    "                X = xr.DataArray(X,\n",
    "                                 dims=['sampling', 'n_features'],\n",
    "                                 coords={'sampling': range(0, X.shape[0]),\n",
    "                                         'n_features': np.arange(0,X.shape[1])})\n",
    "                if self._debug:\n",
    "                    print(\"\\t\", \"X REDUCED with success)\", str(LogDataType(X)))\n",
    "\n",
    "\n",
    "        # Output:\n",
    "        return X, sampling_dims\n",
    "\n",
    "    def preprocessing(self, ds, features=None, dim=None, action='?', mask=None):\n",
    "        \"\"\" Dataset pre-processing of feature(s)\n",
    "\n",
    "        Depending on pyXpcm set-up, pre-processing steps can be:\n",
    "\n",
    "        - interpolation,\n",
    "        - scaling,\n",
    "        - reduction\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ds: :class:`xarray.Dataset`\n",
    "            The dataset to work with\n",
    "\n",
    "        features: dict()\n",
    "            Definitions of PCM features in the input :class:`xarray.Dataset`.\n",
    "            If not specified or set to None, features are identified using :class:`xarray.DataArray` attributes 'feature_name'.\n",
    "\n",
    "        dim: str\n",
    "            Name of the vertical dimension in the input :class:`xarray.Dataset`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X: np.array\n",
    "            Pre-processed set of features, with dimensions (N_SAMPLE, N_FEATURES)\n",
    "\n",
    "        sampling_dims: list()\n",
    "            List of the input :class:`xarray.Dataset` dimensions stacked as sampling points\n",
    "\n",
    "        \"\"\"\n",
    "        this_context = str(action)+'.1-preprocess'\n",
    "        with self._context(this_context, self._context_args):\n",
    "            if self._debug:\n",
    "                print(\"> Start preprocessing for action '%s'\" % action)\n",
    "\n",
    "            # How do we find feature variable in this dataset ?\n",
    "            features_dict = ds.pyxpcm.feature_dict(self, features=features)\n",
    "\n",
    "            # Determine mask where all features are defined for this PCM:\n",
    "            with self._context(this_context + '.1-mask', self._context_args):\n",
    "                if not mask:\n",
    "                    mask = ds.pyxpcm.mask(self, features=features, dim=dim)\n",
    "                    # Stack all-features mask:\n",
    "                    mask = mask.stack({'sampling': list(mask.dims)})\n",
    "                self._xmask = mask\n",
    "\n",
    "            # Pre-process all features and build the X array\n",
    "            X = np.empty(())\n",
    "            Xlabel = list() # Construct a list of string labels for each feature dimension (useful for plots)\n",
    "            F = self.F # Nb of features\n",
    "\n",
    "            for feature_in_pcm in features_dict:\n",
    "                feature_in_ds = features_dict[feature_in_pcm]\n",
    "                if self._debug:\n",
    "                    print( (\"\\n\\t> Preprocessing xarray dataset '%s' as PCM feature '%s'\")\\\n",
    "                           %(feature_in_ds, feature_in_pcm) )\n",
    "\n",
    "                if ('maxlevel' in self._context_args) and (self._context_args['maxlevel'] <= 2):\n",
    "                    a = this_context + '.2-features'\n",
    "                else:\n",
    "                    a = this_context\n",
    "                with self._context(a, self._context_args):\n",
    "                    da = ds[feature_in_ds]\n",
    "                    x, sampling_dims = self.preprocessing_this(da,\n",
    "                                                               dim=dim,\n",
    "                                                               feature_name=feature_in_pcm,\n",
    "                                                               action=action)\n",
    "                    xlabel = [\"%s_%i\"%(feature_in_pcm, i) for i in range(0, x.shape[1])]\n",
    "                    if self._debug:\n",
    "                        print(\"\\t%s pre-processed with success, \"  % feature_in_pcm, str(LogDataType(x)))\n",
    "\n",
    "                with self._context(this_context + '.3-homogeniser', self._context_args):\n",
    "                    # Store full array mean and std during fit:\n",
    "                    if F>1:\n",
    "                        # For more than 1 feature, we need to make them comparable,\n",
    "                        # so we normalise each features by their global stats:\n",
    "                        # FIT:\n",
    "                        if (action == 'fit') or (action == 'fit_predict'):\n",
    "                            self._homogeniser[feature_in_pcm]['mean'] = x.mean().values\n",
    "                            self._homogeniser[feature_in_pcm]['std'] = x.std().values\n",
    "                            #todo _homogeniser should be a proper standard scaler\n",
    "                        # TRANSFORM:\n",
    "                        x = (x-self._homogeniser[feature_in_pcm]['mean'])/\\\n",
    "                            self._homogeniser[feature_in_pcm]['std']\n",
    "                        if self._debug and action == 'fit':\n",
    "                            print((\"\\tHomogenisation for fit of %s\") % (feature_in_pcm))\n",
    "                        elif self._debug:\n",
    "                            print((\"\\tHomogenisation of %s using fit data\") % (feature_in_pcm))\n",
    "                    elif self._debug:\n",
    "                        print((\"\\tNo need for homogenisation of %s\") % (feature_in_pcm))\n",
    "\n",
    "                if np.prod(X.shape) == 1:\n",
    "                    X = x\n",
    "                    Xlabel = xlabel\n",
    "                else:\n",
    "                    X = np.append(X, x, axis=1)\n",
    "                    [Xlabel.append(i) for i in xlabel]\n",
    "\n",
    "            with self._context(this_context + '.4-xarray', self._context_args):\n",
    "                self._xlabel = Xlabel\n",
    "                if self._debug:\n",
    "                    print(\"\\tFeatures array shape and type for xarray:\",\n",
    "                          X.shape, type(X), type(X.data))\n",
    "                X = xr.DataArray(X, dims=['n_samples', 'n_features'],\n",
    "                                 coords={'n_samples': range(0, X.shape[0]), 'n_features': Xlabel})\n",
    "\n",
    "            if self._debug:\n",
    "                print(\"> Preprocessing done, working with final X (%s) array of shape:\" % type(X), X.shape,\n",
    "                      \" and sampling dimensions:\", sampling_dims)\n",
    "        return X, sampling_dims\n",
    "\n",
    "    def fit(self, ds, features=None, dim=None):\n",
    "        \"\"\"Estimate PCM parameters\n",
    "\n",
    "        For a PCM, the fit method consists in the following operations:\n",
    "\n",
    "        - pre-processing\n",
    "            - interpolation to the ``feature_axis`` levels of the model\n",
    "            - scaling\n",
    "            - reduction\n",
    "        - estimate classifier parameters\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ds: :class:`xarray.Dataset`\n",
    "            The dataset to work with\n",
    "\n",
    "        features: dict()\n",
    "            Definitions of PCM features in the input :class:`xarray.Dataset`.\n",
    "            If not specified or set to None, features are identified using :class:`xarray.DataArray` attributes 'feature_name'.\n",
    "\n",
    "        dim: str\n",
    "            Name of the vertical dimension in the input :class:`xarray.Dataset`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        with self._context('fit', self._context_args) :\n",
    "            # PRE-PROCESSING:\n",
    "            X, sampling_dims = self.preprocessing(ds, features=features, dim=dim, action='fit')\n",
    "\n",
    "            # CLASSIFICATION-MODEL TRAINING:\n",
    "            with self._context('fit.fit', self._context_args):\n",
    "                self._classifier.fit(X)\n",
    "\n",
    "            with self._context('fit.score', self._context_args):\n",
    "                self._props['llh'] = self._classifier.score(X)\n",
    "\n",
    "            # Furthermore gather some information about the fit:\n",
    "            self._fit_stats['score'] = self._props['llh']\n",
    "            self._fit_stats['datetime'] = datetime.utcnow()\n",
    "            if 'n_samples_seen_' not in self._classifier.__dict__:\n",
    "                self._fit_stats['n_samples_seen_'] = X.shape[0]\n",
    "            else:\n",
    "                self._fit_stats['n_samples_seen_'] = self._classifier.n_samples_seen_\n",
    "            if 'n_iter_' in self._classifier.__dict__:\n",
    "                self._fit_stats['n_iter_'] = self._classifier.n_iter_\n",
    "\n",
    "        # Done:\n",
    "        self.fitted = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, ds, features=None, dim=None, inplace=False, name='PCM_LABELS'):\n",
    "        \"\"\"Predict labels for profile samples\n",
    "\n",
    "        This method add these properties to the PCM object:\n",
    "\n",
    "        - ``llh``: The log likelihood of the model with regard to new data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ds: :class:`xarray.Dataset`\n",
    "            The dataset to work with\n",
    "\n",
    "        features: dict()\n",
    "            Definitions of PCM features in the input :class:`xarray.Dataset`.\n",
    "            If not specified or set to None, features are identified using :class:`xarray.DataArray` attributes 'feature_name'.\n",
    "\n",
    "        dim: str\n",
    "            Name of the vertical dimension in the input :class:`xarray.Dataset`\n",
    "\n",
    "        inplace: boolean, False by default\n",
    "            If False, return a :class:`xarray.DataArray` with predicted labels\n",
    "            If True, return the input :class:`xarray.Dataset` with labels added as a new :class:`xarray.DataArray`\n",
    "\n",
    "        name: str, default is 'PCM_LABELS'\n",
    "            Name of the :class:`xarray.DataArray` with labels\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        :class:`xarray.DataArray`\n",
    "            Component labels (if option 'inplace' = False)\n",
    "\n",
    "        *or*\n",
    "\n",
    "        :class:`xarray.Dataset`\n",
    "            Input dataset with Component labels as a 'PCM_LABELS' new :class:`xarray.DataArray`\n",
    "            (if option 'inplace' = True)\n",
    "        \"\"\"\n",
    "        with self._context('predict', self._context_args):\n",
    "            # Check if the PCM is trained:\n",
    "            validation.check_is_fitted(self, 'fitted')\n",
    "\n",
    "            # PRE-PROCESSING:\n",
    "            X, sampling_dims = self.preprocessing(ds, features=features, dim=dim, action='predict')\n",
    "\n",
    "            # CLASSIFICATION PREDICTION:\n",
    "            with self._context('predict.predict', self._context_args):\n",
    "                labels = self._classifier.predict(X)\n",
    "            with self._context('predict.score', self._context_args):\n",
    "                llh = self._classifier.score(X)\n",
    "\n",
    "            # Create a xarray with labels output:\n",
    "            with self._context('predict.xarray', self._context_args):\n",
    "                da = self.unravel(ds, sampling_dims, labels).rename(name)\n",
    "                da.attrs['long_name'] = 'PCM labels'\n",
    "                da.attrs['units'] = ''\n",
    "                da.attrs['valid_min'] = 0\n",
    "                da.attrs['valid_max'] = self._props['K']-1\n",
    "                da.attrs['llh'] = llh\n",
    "\n",
    "            # Add labels to the dataset:\n",
    "            if inplace:\n",
    "                return ds.pyxpcm.add(da)\n",
    "            else:\n",
    "                return da\n",
    "\n",
    "    def fit_predict(self, ds, features=None, dim=None, inplace=False, name='PCM_LABELS'):\n",
    "        \"\"\"Estimate PCM parameters and predict classes.\n",
    "\n",
    "        This method add these properties to the PCM object:\n",
    "\n",
    "        - ``llh``: The log likelihood of the model with regard to new data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ds: :class:`xarray.Dataset`\n",
    "            The dataset to work with\n",
    "\n",
    "        features: dict()\n",
    "            Definitions of PCM features in the input :class:`xarray.Dataset`.\n",
    "            If not specified or set to None, features are identified using :class:`xarray.DataArray` attributes 'feature_name'.\n",
    "\n",
    "        dim: str\n",
    "            Name of the vertical dimension in the input :class:`xarray.Dataset`\n",
    "\n",
    "        inplace: boolean, False by default\n",
    "            If False, return a :class:`xarray.DataArray` with predicted labels\n",
    "            If True, return the input :class:`xarray.Dataset` with labels added as a new :class:`xarray.DataArray`\n",
    "\n",
    "        name: string ('PCM_LABELS')\n",
    "            Name of the DataArray holding labels.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        :class:`xarray.DataArray`\n",
    "            Component labels (if option 'inplace' = False)\n",
    "\n",
    "        *or*\n",
    "\n",
    "        :class:`xarray.Dataset`\n",
    "            Input dataset with component labels as a 'PCM_LABELS' new :class:`xarray.DataArray` (if option 'inplace' = True)\n",
    "\n",
    "        \"\"\"\n",
    "        with self._context('fit_predict', self._context_args):\n",
    "\n",
    "            # PRE-PROCESSING:\n",
    "            X, sampling_dims = self.preprocessing(ds, features=features, dim=dim, action='fit_predict')\n",
    "\n",
    "            # CLASSIFICATION-MODEL TRAINING:\n",
    "            with self._context('fit_predict.fit', self._context_args):\n",
    "                self._classifier.fit(X)\n",
    "            with self._context('fit_predict.score', self._context_args):\n",
    "                self._props['llh'] = self._classifier.score(X)\n",
    "\n",
    "            # Furthermore gather some information about this fit:\n",
    "            self._fit_stats['score'] = self._props['llh']\n",
    "            if 'n_samples_seen_' not in self._classifier.__dict__:\n",
    "                self._fit_stats['n_samples_seen_'] = X.shape[0]\n",
    "            else:\n",
    "                self._fit_stats['n_samples_seen_'] = self._classifier.n_samples_seen_\n",
    "            if 'n_iter_' in self._classifier.__dict__:\n",
    "                self._fit_stats['n_iter_'] = self._classifier.n_iter_\n",
    "\n",
    "            # Done:\n",
    "            self.fitted = True\n",
    "\n",
    "            # CLASSIFICATION PREDICTION:\n",
    "            with self._context('fit_predict.predict', self._context_args):\n",
    "                labels = self._classifier.predict(X)\n",
    "\n",
    "            # Create a xarray with labels output:\n",
    "            with self._context('fit_predict.xarray', self._context_args):\n",
    "                da = self.unravel(ds, sampling_dims, labels).rename(name)\n",
    "                da.attrs['long_name'] = 'PCM labels'\n",
    "                da.attrs['units'] = ''\n",
    "                da.attrs['valid_min'] = 0\n",
    "                da.attrs['valid_max'] = self._props['K']-1\n",
    "                da.attrs['llh'] = self._props['llh']\n",
    "\n",
    "            # Add labels to the dataset:\n",
    "            if inplace:\n",
    "                return ds.pyxpcm.add(da)\n",
    "            else:\n",
    "                return da\n",
    "\n",
    "    def predict_proba(self, ds, features=None, dim=None, inplace=False, name='PCM_POST', classdimname='pcm_class'):\n",
    "        \"\"\"Predict posterior probability of each components given the data\n",
    "\n",
    "        This method adds these properties to the PCM instance:\n",
    "\n",
    "        - ``llh``: The log likelihood of the model with regard to new data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ds: :class:`xarray.Dataset`\n",
    "            The dataset to work with\n",
    "\n",
    "        features: dict()\n",
    "            Definitions of PCM features in the input :class:`xarray.Dataset`.\n",
    "            If not specified or set to None, features are identified using :class:`xarray.DataArray` attributes 'feature_name'.\n",
    "\n",
    "        dim: str\n",
    "            Name of the vertical dimension in the input :class:`xarray.Dataset`\n",
    "\n",
    "        inplace: boolean, False by default\n",
    "            If False, return a :class:`xarray.DataArray` with predicted probabilities\n",
    "            If True, return the input :class:`xarray.Dataset` with probabilities added as a new :class:`xarray.DataArray`\n",
    "\n",
    "        name: str, default is 'PCM_POST'\n",
    "            Name of the DataArray with prediction probability (posteriors)\n",
    "\n",
    "        classdimname: str, default is 'pcm_class'\n",
    "            Name of the dimension holding classes\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        :class:`xarray.DataArray`\n",
    "            Probability of each Gaussian (state) in the model given each\n",
    "            sample (if option 'inplace' = False)\n",
    "\n",
    "        *or*\n",
    "\n",
    "        :class:`xarray.Dataset`\n",
    "            Input dataset with Component Probability as a 'PCM_POST' new :class:`xarray.DataArray`\n",
    "            (if option 'inplace' = True)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        with self._context('predict_proba', self._context_args):\n",
    "\n",
    "            # Check if the PCM is trained:\n",
    "            validation.check_is_fitted(self, 'fitted')\n",
    "\n",
    "            # PRE-PROCESSING:\n",
    "            X, sampling_dims = self.preprocessing(ds, features=features, dim=dim, action='predict_proba')\n",
    "\n",
    "            # CLASSIFICATION PREDICTION:\n",
    "            with self._context('predict_proba.predict', self._context_args):\n",
    "                post_values = self._classifier.predict_proba(X)\n",
    "            with self._context('predict_proba.score', self._context_args):\n",
    "                self._props['llh'] = self._classifier.score(X)\n",
    "\n",
    "            # Create a xarray with posteriors:\n",
    "            with self._context('predict_proba.xarray', self._context_args):\n",
    "                P = list()\n",
    "                for k in range(self.K):\n",
    "                    X = post_values[:, k]\n",
    "                    x = self.unravel(ds, sampling_dims, X)\n",
    "                    P.append(x)\n",
    "                da = xr.concat(P, dim=classdimname).rename(name)\n",
    "                da.attrs['long_name'] = 'PCM posteriors'\n",
    "                da.attrs['units'] = ''\n",
    "                da.attrs['valid_min'] = 0\n",
    "                da.attrs['valid_max'] = 1\n",
    "                da.attrs['llh'] = self._props['llh']\n",
    "\n",
    "            # Add posteriors to the dataset:\n",
    "            if inplace:\n",
    "                return ds.pyxpcm.add(da)\n",
    "            else:\n",
    "                return da\n",
    "\n",
    "    def score(self, ds, features=None, dim=None):\n",
    "        \"\"\"Compute the per-sample average log-likelihood of the given data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ds: :class:`xarray.Dataset`\n",
    "            The dataset to work with\n",
    "\n",
    "        features: dict()\n",
    "            Definitions of PCM features in the input :class:`xarray.Dataset`.\n",
    "            If not specified or set to None, features are identified using :class:`xarray.DataArray` attributes 'feature_name'.\n",
    "\n",
    "        dim: str\n",
    "            Name of the vertical dimension in the input :class:`xarray.Dataset`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        log_likelihood: float\n",
    "            In the case of a GMM classifier, this is the Log likelihood of the Gaussian mixture given data\n",
    "\n",
    "        \"\"\"\n",
    "        with self._context('score', self._context_args):\n",
    "\n",
    "            # Check if the PCM is trained:\n",
    "            validation.check_is_fitted(self, 'fitted')\n",
    "\n",
    "            # PRE-PROCESSING:\n",
    "            X, sampling_dims = self.preprocessing(ds, features=features, action='score')\n",
    "\n",
    "            # COMPUTE THE PREDICTION SCORE:\n",
    "            with self._context('score.score', self._context_args):\n",
    "                llh = self._classifier.score(X)\n",
    "\n",
    "        return llh\n",
    "\n",
    "    def bic(self, ds, features=None, dim=None):\n",
    "        \"\"\"Compute Bayesian information criterion for the current model on the input dataset\n",
    "\n",
    "        Only for a GMM classifier\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ds: :class:`xarray.Dataset`\n",
    "            The dataset to work with\n",
    "\n",
    "        features: dict()\n",
    "            Definitions of PCM features in the input :class:`xarray.Dataset`.\n",
    "            If not specified or set to None, features are identified using :class:`xarray.DataArray` attributes 'feature_name'.\n",
    "\n",
    "        dim: str\n",
    "            Name of the vertical dimension in the input :class:`xarray.Dataset`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bic: float\n",
    "            The lower the better\n",
    "        \"\"\"\n",
    "        with self._context('bic', self._context_args):\n",
    "\n",
    "            # Check classifier:\n",
    "            if self._props['with_classifier'] != 'gmm':\n",
    "                raise Exception( (\"BIC is only available for the 'gmm' classifier ('%s')\")%\\\n",
    "                                 (self._props['with_classifier']) )\n",
    "\n",
    "            def _n_parameters(_classifier):\n",
    "                \"\"\"Return the number of free parameters in the model. See sklearn code\"\"\"\n",
    "                _, n_features = _classifier.means_.shape\n",
    "                if _classifier.covariance_type == 'full':\n",
    "                    cov_params = _classifier.n_components * n_features * (n_features + 1) / 2.\n",
    "                elif _classifier.covariance_type == 'diag':\n",
    "                    cov_params = _classifier.n_components * n_features\n",
    "                elif _classifier.covariance_type == 'tied':\n",
    "                    cov_params = n_features * (n_features + 1) / 2.\n",
    "                elif _classifier.covariance_type == 'spherical':\n",
    "                    cov_params = _classifier.n_components\n",
    "                mean_params = n_features * _classifier.n_components\n",
    "                return int(cov_params + mean_params + _classifier.n_components - 1)\n",
    "\n",
    "            # Check if the PCM is trained:\n",
    "            validation.check_is_fitted(self, 'fitted')\n",
    "\n",
    "            # PRE-PROCESSING:\n",
    "            X, sampling_dims = self.preprocessing(ds, features=features, action='bic')\n",
    "\n",
    "            # COMPUTE THE log-likelihood:\n",
    "            with self._context('bic.score', self._context_args):\n",
    "                llh = self._classifier.score(X)\n",
    "\n",
    "            # COMPUTE BIC:\n",
    "            N_samples = X.shape[0]\n",
    "            bic = (-2 * llh * N_samples + _n_parameters(self._classifier) * np.log(N_samples))\n",
    "\n",
    "        return bic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:BlueCloud]",
   "language": "python",
   "name": "conda-env-BlueCloud-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
